# MNIST Digit Classification using a GPU-accelerated Neural Network 

The MNIST data set consists of 60,000 grayscale 28Ã—28 pixel images of handwritten digits. Each image has an associated label, indicating the correct digit. In our model, we pass in the image data as a flattened vector of length 784. To classify the digit, we output a length 10 vector representing each digit's predicted probability. For simplicity, we use a 2-layer network, consisting of a hidden layer of size variable size and an output layer of size 10. We apply a ReLU activation on the hidden layer and a softmax activation for the output layer. For our loss function, we use cross-entropy. We train our model using 37800 training images and evaluate our prediction accuracy using 4200 images. 


In our experiments, we measure the training time of each model for 5 epochs. We vary the size of the hidden layer to investigate how the size of the model influences runtime and speedup from using a GPU. We also vary the batch size, which is how much training data is fed into the model each time before performing a weight update based on the error of the batch. We hold all other variables except the learning rate constant. However, note that the learning rate has no impact on the training time, only on model convergence. We also record the trained model accuracy on the test data to verify that the models are indeed learning. In terms of evaluating performance, we are only interested in the training time rather than test accuracy. We expect that using larger hidden layer sizes and larger batch sizes correspond to worse test accuracy. This is because we only make 5 passes through the training data, so a larger model will be less trained and a larger batch size results in fewer weight updates. In real-world applications, we would simply train for a greater number of epochs to improve the performance of larger models. We do not increase epochs here for the sake of comparability in terms of training time.
